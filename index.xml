<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Oleg Baydakov</title>
    <link>https://obaydakov.github.io/</link>
    <description>Recent content on Oleg Baydakov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Sep 2017 22:10:15 +0000</lastBuildDate>
    
	<atom:link href="https://obaydakov.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mac OS X Setup Guide</title>
      <link>https://obaydakov.github.io/post/2017/mac-os-x-setup-guide/</link>
      <pubDate>Sat, 16 Sep 2017 22:10:15 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/mac-os-x-setup-guide/</guid>
      <description>This guide covers the basics of setting up a development environment on a new Mac. Whether you are an experienced programmer or not, this guide is intended for everyone to use as a reference for setting up your environment or installing a languages/libraries.</description>
    </item>
    
    <item>
      <title>Generative Adversarial Networks (GANs): Engine and Applications</title>
      <link>https://obaydakov.github.io/post/2017/generative-adversarial-networks-gans-engine-and-applications/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/generative-adversarial-networks-gans-engine-and-applications/</guid>
      <description>Generative adversarial networks (GANs) are a class of neural networks that are used in unsupervised machine learning. They help to solve such tasks as image generation from descriptions, getting high resolution images from low resolution ones, predicting which drug could treat a certain disease, retrieving images that contain a given pattern, etc.</description>
    </item>
    
    <item>
      <title>Golang development</title>
      <link>https://obaydakov.github.io/post/2017/golang-development/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/golang-development/</guid>
      <description>Video</description>
    </item>
    
    <item>
      <title>Visualize patients&#39; complaints to their doctors using NiFi and Solr/Banana</title>
      <link>https://obaydakov.github.io/post/2017/visualize-patients-complaints-to-their-doctors-using-nifi-and-solr-banana/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/visualize-patients-complaints-to-their-doctors-using-nifi-and-solr-banana/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Adventures in Machine Learning New</title>
      <link>https://obaydakov.github.io/post/2017/adventures-in-machine-learning/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/adventures-in-machine-learning/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Awesome R</title>
      <link>https://obaydakov.github.io/post/2017/awesome-r/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/awesome-r/</guid>
      <description>A curated list of awesome R frameworks, libraries and software.
Link</description>
    </item>
    
    <item>
      <title>High Level Design of Analytical system based on R</title>
      <link>https://obaydakov.github.io/post/2017/high-level-design-of-analytical-system-based-on-r/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/high-level-design-of-analytical-system-based-on-r/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ScalNet - A Scala wrapper for Deeplearning4j, inspired by Keras. Scala &#43; DL &#43; Spark &#43; GPUs</title>
      <link>https://obaydakov.github.io/post/2017/scalnet-a-scala-wrapper-for-deeplearning4j-inspired-by-keras-scala-dl-spark-gpus/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/scalnet-a-scala-wrapper-for-deeplearning4j-inspired-by-keras-scala-dl-spark-gpus/</guid>
      <description>ScalNet is a wrapper around Deeplearning4j emulating a Keras like API for deep learning. ScalNet is released under an Apache 2.0 license. By contributing code to this repository, you agree to make your contribution available under an Apache 2.0 license.
Link</description>
    </item>
    
    <item>
      <title>An overview of gradient descent optimization algorithms</title>
      <link>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</guid>
      <description>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne&amp;rsquo;s, caffe&amp;rsquo;s, and keras&amp;rsquo; documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.
Link</description>
    </item>
    
    <item>
      <title>Deep Learning Model Zoo</title>
      <link>https://obaydakov.github.io/post/2017/deep-learning-model-zoo/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-learning-model-zoo/</guid>
      <description>A collection of standalone TensorFlow models in Jupyter Notebooks
Link
Free Chapters from Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python</description>
    </item>
    
    <item>
      <title>Model evaluation, model selection, and algorithm selection in machine learning  - Cross-validation and hyperparameter tuning </title>
      <link>https://obaydakov.github.io/post/2017/model-evaluation-model-selection-and-algorithm-selection-in-machine-learning-cross-validation-and-hyperparameter-tuning/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/model-evaluation-model-selection-and-algorithm-selection-in-machine-learning-cross-validation-and-hyperparameter-tuning/</guid>
      <description>Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset.</description>
    </item>
    
    <item>
      <title>Awesome R Shiny</title>
      <link>https://obaydakov.github.io/post/2017/awesome-r-shiny/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/awesome-r-shiny/</guid>
      <description>A curated list of resources for R Shiny
Link</description>
    </item>
    
    <item>
      <title>Crosstalk</title>
      <link>https://obaydakov.github.io/post/2017/crosstalk/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/crosstalk/</guid>
      <description>Crosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).
Play with the example below by manipulating the slider, clicking rows in the data table, and playing with the selection button in the map.
Link</description>
    </item>
    
    <item>
      <title>Docker for Data Scientists</title>
      <link>https://obaydakov.github.io/post/2017/docker-for-data-scientists/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/docker-for-data-scientists/</guid>
      <description>Data scientists inhabit such an ever-changing landscape of languages, packages, and frameworks that it can be easy to succumb to tool fatigue. If this sounds familiar, you may have missed the increasing popularity of Linux containers in the DevOps world, in particular Docker. Michelangelo D&amp;rsquo;Agostino demonstrates Docker&amp;rsquo;s many benefits, from making data science code and environments more portable and shareable to making the transition from development to production more seamless to giving data scientists a common basis for collaborating with software engineers, and explains why Docker deserves a place in every data scientist&amp;rsquo;s toolkit.</description>
    </item>
    
    <item>
      <title>Random Bot for OpenML platform</title>
      <link>https://obaydakov.github.io/post/2017/random-bot-for-openml-platform/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/random-bot-for-openml-platform/</guid>
      <description>&amp;lsquo;RandomBot&amp;rsquo; is a program that searches randomly the hyper-parameter space from different Machine Learning (ML) algorithms, sampling different hyper-parameter settings via hyper-parameter sweep. It executes pre-defined &amp;lsquo;mlr&amp;rsquo; [1] learners over OpenML [2] classification problems using the &amp;lsquo;BatchExperiments&amp;rsquo; package [3] structure.
For each pair of {task, algorithm}, the bot will start N new jobs with random different hper-parameter settings. The budget N is defined as N = 100 * D , where D denotes the number of parameters in that algorithm&amp;rsquo;s space, and 100 is the tuning constant (defined empirically).</description>
    </item>
    
    <item>
      <title>Trelliscope</title>
      <link>https://obaydakov.github.io/post/2017/trelliscope/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/trelliscope/</guid>
      <description>Trelliscope is a visualization approach based on the idea of &amp;ldquo;small multiples&amp;rdquo; or &amp;ldquo;Trellis Display&amp;rdquo;, where data are split into groups and a plot is made for each group, with the resulting plots arranged in a grid. This approach is very simple yet is considered to be &amp;ldquo;the best design solution for a wide range or problems in data presentation&amp;rdquo;. Trelliscope makes small multiple displays come alive by providing the ability to interactively sort and filter the plots based on summary statistics computed for each group.</description>
    </item>
    
    <item>
      <title>Web Sites with R</title>
      <link>https://obaydakov.github.io/post/2017/web-sites-with-r/</link>
      <pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/web-sites-with-r/</guid>
      <description>Blogdown (R package)
blogdown ??? Hugo + R Markdown
Hugo can only process Markdown; R Markdown is prebuilt to .html and passed to Hugo a few helper functions
new_site() creates a new website
install_hugo() / install_theme()
serve_site() rebuilds and live reloads a website on changes (RStudio addin &amp;ldquo;Live Preview Site&amp;rdquo;)
new_post() creates a new post under content/post/ and automatically fills out some YAML metadata (title, author, date, &amp;hellip;)
output format: blogdown::html_page</description>
    </item>
    
    <item>
      <title>Introduction to parallel computing with R</title>
      <link>https://obaydakov.github.io/post/2017/introduction-to-parallel-computing-with-r/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/introduction-to-parallel-computing-with-r/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>OpenML: Connecting R to the Machine Learning Platform OpenML</title>
      <link>https://obaydakov.github.io/post/2017/openml-connecting-r-to-the-machine-learning-platform-openml/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/openml-connecting-r-to-the-machine-learning-platform-openml/</guid>
      <description>Tutorial 1
Tutorial 2
Slides
R interface to OpenML
Tutorial 3
Tutorial 4</description>
    </item>
    
    <item>
      <title>Package ggiraph: a ggplot2 Extension for Interactive Graphics</title>
      <link>https://obaydakov.github.io/post/2017/package-ggiraph-a-ggplot2-extension-for-interactive-graphics/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/package-ggiraph-a-ggplot2-extension-for-interactive-graphics/</guid>
      <description>ggiraph is an htmlwidget and a ggplot2 extension. It lets ggplot graphics being animated.
Animation is made with ggplot geometries that can understand three arguments:
tooltip: column of dataset that contains tooltips to be displayed when mouse is over elements.
onclick: column of dataset that contains javascript function to be executed when elements are clicked.
data_id: column of dataset that contains id to be associated with elements.
Link
Video</description>
    </item>
    
    <item>
      <title>Productionizing Apache SparkT MLlib Models for Real-time Prediction Serving</title>
      <link>https://obaydakov.github.io/post/2017/c-users-oleg-baydakov-documents-myblog-new-myblog-hugo/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/c-users-oleg-baydakov-documents-myblog-new-myblog-hugo/</guid>
      <description>Data science and machine learning tools traditionally focus on training models. When companies begin to employ machine learning in actual production workflows, they encounter new sources of friction such as sharing models across teams, deploying identical models on different systems, and maintaining featurization logic. In this webinar, we discuss how Databricks provides a smooth path for productionizing Apache Spark MLlib models and featurization pipelines.
Databricks Model Scoring provides a simple API for exporting MLlib models and pipelines.</description>
    </item>
    
    <item>
      <title>jug: A Simple Web Framework for R</title>
      <link>https://obaydakov.github.io/post/2017/jug-a-simple-web-framework-for-r/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/jug-a-simple-web-framework-for-r/</guid>
      <description>jug is a web framework aimed at easily building APIs. It is mostly aimed at exposing R functions, models and visualizations to third-parties by way of http requests.
Link
Video</description>
    </item>
    
    <item>
      <title>shiny.semantic</title>
      <link>https://obaydakov.github.io/post/2017/shiny-semantic/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/shiny-semantic/</guid>
      <description>Semantic UI wrapper for Shiny
With this library it&amp;rsquo;s easy to wrap Shiny with Semantic UI components. Add a few simple lines of code and some CSS classes to give your UI a fresh, modern and highly interactive look.
Link
Video</description>
    </item>
    
    <item>
      <title>iheatmapr-  R package</title>
      <link>https://obaydakov.github.io/post/2017/iheatmapr-r-package/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/iheatmapr-r-package/</guid>
      <description>iheatmapr is an R package for building complex, interactive heatmaps using modular building blocks. &amp;ldquo;Complex&amp;rdquo; heatmaps are heatmaps in which subplots along the rows or columns of the main heatmap add more information about each row or column. For example, a one column additional heatmap may indicate what group a particular row or column belongs to. Complex heatmaps may also include multiple side by side heatmaps which show different types of data for the same conditions.</description>
    </item>
    
    <item>
      <title>Deep Learning Limitations</title>
      <link>https://obaydakov.github.io/post/2017/deep-learning-limitations/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-learning-limitations/</guid>
      <description>Limitations of deep learning and future</description>
    </item>
    
    <item>
      <title>Kali Linux</title>
      <link>https://obaydakov.github.io/post/2017/kali-linux/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/kali-linux/</guid>
      <description>Good tutorials
Link
Tools
Ethical hacking
Hacking with Google</description>
    </item>
    
    <item>
      <title>Machine Learning Cheatsheets</title>
      <link>https://obaydakov.github.io/post/2017/machine-learning-cheatsheets/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/machine-learning-cheatsheets/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>An overview of gradient descent optimization algorithms</title>
      <link>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</guid>
      <description>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne&amp;rsquo;s, caffe&amp;rsquo;s, and keras&amp;rsquo; documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.
Link</description>
    </item>
    
    <item>
      <title>Apache Metron - Cyber Security on Big Data</title>
      <link>https://obaydakov.github.io/post/2017/apache-metron-cyber-security-on-big-data/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/apache-metron-cyber-security-on-big-data/</guid>
      <description>WHAT APACHE METRON DOES
Apache Metron provides a scalable advanced security analytics framework built with the Hadoop Community evolving from the Cisco OpenSOC Project. A cyber security application framework that provides organizations the ability to detect cyber anomalies and enable organizations to rapidly respond to identified anomalies.
Link</description>
    </item>
    
    <item>
      <title>Python framewrok - QML</title>
      <link>https://obaydakov.github.io/post/2017/python-framewrok-qml/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/python-framewrok-qml/</guid>
      <description>Description
Code</description>
    </item>
    
    <item>
      <title>Real-Time Weather Event Processing With HDF, Spark Streaming, and Solr</title>
      <link>https://obaydakov.github.io/post/2017/real-time-weather-event-processing-with-hdf-spark-streaming-and-solr/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/real-time-weather-event-processing-with-hdf-spark-streaming-and-solr/</guid>
      <description>HDF collects, curates, analyzes, and delivers real-time data to data stores quickly and easily. It can be used with Spark Streaming and Solr to process weather events.
Link</description>
    </item>
    
    <item>
      <title>CatBoost</title>
      <link>https://obaydakov.github.io/post/2017/catboost/</link>
      <pubDate>Tue, 18 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/catboost/</guid>
      <description>CatBoost is a state-of-the-art open-source gradient boosting on decision trees library.
Developed by Yandex researchers and engineers, it is the successor of the MatrixNet algorithm that is widely used within the company for ranking tasks, forecasting and making recommendations. It is universal and can be applied across a wide range of areas and to a variety of problems.
Accurate: leads or ties competition on standard benchmarks Robust: reduces the need for extensive hyper-parameter tuning Easy-to-use: offers Python interfaces integrated with scikit, as well as R and command-line interfaces Practical: uses categorical features directly and scalably Extensible: allows specifying custom loss functions</description>
    </item>
    
    <item>
      <title>Credit Risk Prediction</title>
      <link>https://obaydakov.github.io/post/2017/credit-risk-prediction/</link>
      <pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/credit-risk-prediction/</guid>
      <description>Credit Risk Scoring is a classic yet still increasingly important operation in banking as banks continue to be increasingly risk careful when lending for mortgages or commercial purposes, in an industry known for fierce competition and the global financial crisis. With an accurate credit risk scoring model a bank is able to predict the likelihood of default on a transaction. This will in turn help evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt, as well as determine who qualifies for a loan, at what interest rate, and what credit limits, and even determine which customers are likely to bring in the most revenue through a variety of products.</description>
    </item>
    
    <item>
      <title>Deep Learning and NLP</title>
      <link>https://obaydakov.github.io/post/2017/deep-learning-and-nlp/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-learning-and-nlp/</guid>
      <description>Link A news-analysis NeuralNet learns from a language NeuralNet</description>
    </item>
    
    <item>
      <title>R package MLR examples</title>
      <link>https://obaydakov.github.io/post/2017/r-package-mlr-examples/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/r-package-mlr-examples/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Comprehensive list of activation functions in neural networks with pros/cons</title>
      <link>https://obaydakov.github.io/post/2017/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Formattable data frame</title>
      <link>https://obaydakov.github.io/post/2017/formattable-data-frame/</link>
      <pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/formattable-data-frame/</guid>
      <description>Formattable data frames are data frames to be rendered as HTML table with formatter functions applied, which resembles conditional formatting in Microsoft Excel.
Article
Link Github</description>
    </item>
    
    <item>
      <title>Integrating Akka Streams and Akka Actors</title>
      <link>https://obaydakov.github.io/post/2017/integrating-akka-streams-and-akka-actors/</link>
      <pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/integrating-akka-streams-and-akka-actors/</guid>
      <description>In part one of this series, I described the challenges of integrating the Akka Streams API with Akka Actors. I demonstrated the basic patterns for interfacing streams and actors, removing the discontinuity between the two. In this article, I will begin to explore more sophisticated ways to integrate actors and streams, in support of building robust and scalable distributed systems, rather than just simple applications, or stand-alone data-processing pipelines. This article will focus on how Akka Actors compliment the Akka Streams API with regard life-cycle management and fault tolerance.</description>
    </item>
    
    <item>
      <title>Data Lineage Tracking and Visualization tool for Apache Spark</title>
      <link>https://obaydakov.github.io/post/2017/data-lineage-tracking-and-visualization-tool-for-apache-spark/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/data-lineage-tracking-and-visualization-tool-for-apache-spark/</guid>
      <description>Spline (from Spark lineage) project helps people get insight into data processing performed by Apache Spark.
The project consists of two parts:
A core library that sits on drivers, capturing data lineages from the jobs being executed by analyzing Spark execution plans and a Web UI application that visualizes the stored data lineages.
Article</description>
    </item>
    
    <item>
      <title>Convolutional Neural Network</title>
      <link>https://obaydakov.github.io/post/2017/convolutional-neural-network/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/convolutional-neural-network/</guid>
      <description>How to make a Convolutional Neural Network in TensorFlow for recognizing handwritten digits from the MNIST data-set. Video Tutorial</description>
    </item>
    
    <item>
      <title>Bayesian Optimization for Hyperparameter Tuning</title>
      <link>https://obaydakov.github.io/post/2017/bayesian-optimization-for-hyperparameter-tuning/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/bayesian-optimization-for-hyperparameter-tuning/</guid>
      <description>Hyperparameter tuning may be one of the most tricky, yet interesting, topics in Machine Learning. For most Machine Learning practitioners, mastering the art of tuning hyperparameters requires not only a solid background in Machine Learning algorithms, but also extensive experience working with real-world datasets.
In this post, I will give an overview of hyperparameters and various approaches of tuning hyperparameters intelligently. In particular, I will explain how we applied Bayesian Optimization to tune hyperparameters of a predictive model on a real-world dataset.</description>
    </item>
    
    <item>
      <title>Patterns for Streaming Measurement Data with Akka Streams</title>
      <link>https://obaydakov.github.io/post/2017/patterns-for-streaming-measurement-data-with-akka-streams/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/patterns-for-streaming-measurement-data-with-akka-streams/</guid>
      <description>Patterns for Streaming Measurement Data with Akka Streams</description>
    </item>
    
    <item>
      <title>Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</title>
      <link>https://obaydakov.github.io/post/2017/introduction-to-local-interpretable-model-agnostic-explanations-lime/</link>
      <pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/introduction-to-local-interpretable-model-agnostic-explanations-lime/</guid>
      <description>Machine learning is at the core of many recent advances in science and technology. With computers beating professionals in games like Go, many people have started asking if machines would also make for better drivers or even better doctors.
In many applications of machine learning, users are asked to trust a model to help them make decisions. A doctor will certainly not operate on a patient simply because &amp;ldquo;the model said so.</description>
    </item>
    
    <item>
      <title>The Algorithms Behind Probabilistic Programming</title>
      <link>https://obaydakov.github.io/post/2017/the-algorithms-behind-probabilistic-programming/</link>
      <pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/the-algorithms-behind-probabilistic-programming/</guid>
      <description>Bayesian Inference
Probabilistic programming enables us to construct and fit probabilistic models in code. At its essence, Bayesian inference is a principled way to draw conclusions from incomplete or imperfect data, by interpreting data in light of prior knowledge of probabilities. As pretty much all real-world data is incomplete or imperfect in some way, it&amp;rsquo;s an important (and old!) idea.
Bayesian inference might be the way to go if you:</description>
    </item>
    
    <item>
      <title>Transfer Learning - Machine Learning&#39;s Next Frontier</title>
      <link>https://obaydakov.github.io/post/2017/transfer-learning-machine-learning-s-next-frontier/</link>
      <pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/transfer-learning-machine-learning-s-next-frontier/</guid>
      <description>Table of contents:
What is Transfer Learning? Why Transfer Learning Now? A Definition of Transfer Learning Transfer Learning Scenarios Applications of Transfer Learning Learning from simulations Adapting to new domains Transferring knowledge across languages Transfer Learning Methods Using pre-trained CNN features Learning domain-invariant representations Making representations more similar Confusing domains Related Research Areas Semi-supervised learning Using available data more effectively Improving models&amp;rsquo; ability to generalize Making models more robust Multi-task learning Continuous learning Zero-shot learning Conclusion</description>
    </item>
    
    <item>
      <title>Reactive Stream vs Akka Stream</title>
      <link>https://obaydakov.github.io/post/2017/reactive-stream-vs-akka-stream/</link>
      <pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/reactive-stream-vs-akka-stream/</guid>
      <description>Akka Stream Concepts: Source ~&amp;gt; Flow ~&amp;gt; Sink (code example) Akka Stream is built with the decision to offer APIs that are minimal and consistent-as opposed to easy or intuitive. There is no magic, all the API features are explicit. You have operators to define Source, Sink, Flow, Graph and Operators. You have the operators to handle back-pressure, buffering, transformations, failure recovery, etc. But on my opinion the features that make Akka Stream implementation interesting are: 1.</description>
    </item>
    
    <item>
      <title>The Building Blocks Of A CQRS Architected Application</title>
      <link>https://obaydakov.github.io/post/2017/the-building-blocks-of-a-cqrs-architected-application/</link>
      <pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/the-building-blocks-of-a-cqrs-architected-application/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Docker &#43;Akka HTTP</title>
      <link>https://obaydakov.github.io/post/2017/docker-akka-http/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/docker-akka-http/</guid>
      <description>INTRODUCTION
A while ago I worked on a project that used this tech stack
Akka HTTP : (actually we used Spray.IO but it is practically the same thing for the purpose of this article). For those that don&amp;rsquo;t know what Akka HTTP is, it is a simple Akka based framework that is also able to expose a REST interface to communicate with the actor system Cassandra database : Apache Cassandra is a free and open-source distributed database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.</description>
    </item>
    
    <item>
      <title>Apache Kylin  Overview</title>
      <link>https://obaydakov.github.io/post/2017/apache-kylin-overview/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/apache-kylin-overview/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Practical Recommendations for Gradient-Based Training of Deep Architectures</title>
      <link>https://obaydakov.github.io/post/2017/practical-recommendations-for-gradient-based-training-of-deep-architectures/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/practical-recommendations-for-gradient-based-training-of-deep-architectures/</guid>
      <description>Abstract Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyperparameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters.</description>
    </item>
    
    <item>
      <title>Autoencoders and anomaly detection with machine learning in fraud analytics</title>
      <link>https://obaydakov.github.io/post/2017/autoencoders-and-anomaly-detection-with-machine-learning-in-fraud-analytics/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/autoencoders-and-anomaly-detection-with-machine-learning-in-fraud-analytics/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>The Definitive Security Data Science and Machine Learning Guide</title>
      <link>https://obaydakov.github.io/post/2017/the-definitive-security-data-science-and-machine-learning-guide/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/the-definitive-security-data-science-and-machine-learning-guide/</guid>
      <description>This is the Definitive Security Data Science and Machine Learning Guide. It includes books, tutorials, presentations, blog posts, and research papers about solving security problems using data science.
Link
Data Driven Security
Security + Big Data + Data Science</description>
    </item>
    
    <item>
      <title>Favorite Threat Hunting Links</title>
      <link>https://obaydakov.github.io/post/2017/favorite-threat-hunting-links/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/favorite-threat-hunting-links/</guid>
      <description>On UEBA / UBA Use Cases
Favorite Threat Hunting Links
Advances in Cloud-Scale Machine Learning for Cyber-Defense</description>
    </item>
    
    <item>
      <title>Sec Viz - security visualization</title>
      <link>https://obaydakov.github.io/post/2017/sec-viz-security-visualization/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/sec-viz-security-visualization/</guid>
      <description>SecViz - Security Visualization
Link
[Pixcloud] {http://pixlcloud.com/resources/)
Workshop: Big Data Visualization for Security
Periodic Table</description>
    </item>
    
    <item>
      <title>Awesome Scala</title>
      <link>https://obaydakov.github.io/post/2017/awesome-scala/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/awesome-scala/</guid>
      <description>Awesome Scala Awesome
A community driven list of useful Scala libraries, frameworks and software. This is not a catalog of all the libraries, just a starting point for your explorations. Inspired by awesome-python. Other amazingly awesome lists can be found in the awesome-awesomeness list.
Also awesome is Scaladex, the searchable, tagged, and centralized index of Scala libraries.
Awesome Scala</description>
    </item>
    
    <item>
      <title>BigDL: Distributed Deep Learning on Apache Spark</title>
      <link>https://obaydakov.github.io/post/2017/bigdl-distributed-deep-learning-on-apache-spark/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/bigdl-distributed-deep-learning-on-apache-spark/</guid>
      <description>What is BigDL?
BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters.
Rich deep learning support. Modeled after Torch, BigDL provides comprehensive support for deep learning, including numeric computing (via Tensor) and high level neural networks; in addition, users can load pre-trained Caffe or Torch models into Spark programs using BigDL.</description>
    </item>
    
    <item>
      <title>Implementing a custom Akka Streams graph stage </title>
      <link>https://obaydakov.github.io/post/2017/implementing-a-custom-akka-streams-graph-stage/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/implementing-a-custom-akka-streams-graph-stage/</guid>
      <description>THE USE CASE Let&amp;rsquo;s say that having a stream of elements of type E you want to observe their arbitrary property of type P, accumulate the elements as long as the property remains unchanged and only emit an immutable.Seq[E] of accumulated elements when the property changes. In a real-life example the elements can be e.g. lines in a CSV file which you would like to group by a given field.</description>
    </item>
    
    <item>
      <title>Text Mining With Akka Streams</title>
      <link>https://obaydakov.github.io/post/2017/text-mining-with-akka-streams/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/text-mining-with-akka-streams/</guid>
      <description>Extracting n-grams from text
In text mining, n-grams are useful data in the area of NLP (natural language processing). In this blog post, I&amp;rsquo;ll illustrate extracting n-grams from a stream of text messages using Akka Streams with Scala as the programming language.
First thing first, let&amp;rsquo;s create an object with methods for generating random text content
Akka Streams Text Mining</description>
    </item>
    
    <item>
      <title>Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</title>
      <link>https://obaydakov.github.io/post/2017/introduction-to-local-interpretable-model-agnostic-explanations-lime/</link>
      <pubDate>Mon, 24 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/introduction-to-local-interpretable-model-agnostic-explanations-lime/</guid>
      <description>Machine learning is at the core of many recent advances in science and technology. With computers beating professionals in games like Go, many people have started asking if machines would also make for better drivers or even better doctors.
In many applications of machine learning, users are asked to trust a model to help them make decisions. A doctor will certainly not operate on a patient simply because &amp;quot;the model said so.</description>
    </item>
    
    <item>
      <title>Real-World Machine Learning (with R)</title>
      <link>https://obaydakov.github.io/post/2017/real-world-machine-learning-with-r/</link>
      <pubDate>Mon, 24 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/real-world-machine-learning-with-r/</guid>
      <description>Introduction to rwml-R The book &amp;ldquo;Real-World Machine Learning&amp;rdquo; by Henrik Brink, Joseph W. Richards, and Mark Fetherolf attempts to prepare the reader for the realities of machine learning. It covers a basic framework for machine-learning projects, then it dives into extended examples that show how that basic framework can be applied in realistic situations. It attempts to provide the &amp;ldquo;hidden wisdom&amp;rdquo; on how to go about implementing products and solutions based on machine learning.</description>
    </item>
    
    <item>
      <title>How AI drives the mobile contextual revolution</title>
      <link>https://obaydakov.github.io/post/2017/how-ai-drives-the-mobile-contextual-revolution/</link>
      <pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/how-ai-drives-the-mobile-contextual-revolution/</guid>
      <description>In &amp;ldquo;Predictions 2016: The Mobile Revolution Accelerates&amp;rdquo;, Forrester forecasted that by the end of 2016 more than 25% of companies will consider mobile not as a channel, but as a fully integrated part of their overall strategy. Moreover, the research firm expects customer-focused companies to take personalization to the next level by extracting relevant moments from customer data using intelligent data processing techniques:
&amp;ldquo;To serve customers in context where they already are - not where brands find it convenient to serve them, firms must look to use context both to assemble and deliver experiences dynamically&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Modelling Reactive Systems with Event Storming and Domain-Driven Design</title>
      <link>https://obaydakov.github.io/post/2017/modelling-reactive-systems-with-event-storming-and-domain-driven-design/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/modelling-reactive-systems-with-event-storming-and-domain-driven-design/</guid>
      <description>A successful Event Storming session as well as a successful software project demands equal parts art, knowledge, and technical skill. Also, it&amp;rsquo;s much cheaper to make changes to sticky notes than production code. Learning about your systems by writing code is a very expensive way of understanding and refining the business processes involved. Nothing in this article requires technical expertise or previous experience with reactive, event-driven systems. Whether you&amp;rsquo;re a business expert, developer, architect, product owner, or business owner, you should learn a few new techniques that will help you have more success and funcollaborating with your team and organization.</description>
    </item>
    
    <item>
      <title>Message Delivery Reliability</title>
      <link>https://obaydakov.github.io/post/2017/message-delivery-reliability/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/message-delivery-reliability/</guid>
      <description>Akka helps you build reliable applications which make use of multiple processor cores in one machine (&amp;ldquo;scaling up&amp;rdquo;) or distributed across a computer network (&amp;ldquo;scaling out&amp;rdquo;). The key abstraction to make this work is that all interactions between your code units-actors-happen via message passing, which is why the precise semantics of how messages are passed between actors deserve their own chapter.
In order to give some context to the discussion below, consider an application which spans multiple network hosts.</description>
    </item>
    
    <item>
      <title>Algorithms and Data Structures</title>
      <link>https://obaydakov.github.io/post/2017/algorithms-and-data-structures/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/algorithms-and-data-structures/</guid>
      <description>Algorithms and Data Structures
This is the collection of algorithms, data structures and Interview Questions with solutions. This repository contains my solutions for common algorithmic problems and implementation of Data Structures in Java. I&amp;rsquo;ve created this repository to learn about algorithms. I am adding solutions continuously. Link</description>
    </item>
    
    <item>
      <title>Kalman filter</title>
      <link>https://obaydakov.github.io/post/2017/kalman-filter/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/kalman-filter/</guid>
      <description>This article provides a simple and intuitive derivation of the Kalman filter, with the aim of teaching this useful tool to students from disciplines that do not require a strong mathematical background. The most complicated level of mathematics required to understand this derivation is the ability to multiply two Gaussian functions together and reduce the result to a compact form. Link</description>
    </item>
    
    <item>
      <title>Examples of using DL4J on Spark and Scala</title>
      <link>https://obaydakov.github.io/post/2017/examples-of-using-dl4j-on-spark-and-scala/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/examples-of-using-dl4j-on-spark-and-scala/</guid>
      <description>Examples: Project for the talk on NLP using LSTM implementation from DL4J on Spark
Deeplearning4J Examples for Scala
Exploring convolutional neural networks with DL4J
ND4S is open-source Scala bindings for ND4J</description>
    </item>
    
    <item>
      <title>Transfer Learning</title>
      <link>https://obaydakov.github.io/post/2017/transfer-learning/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/transfer-learning/</guid>
      <description>In recent years, we have become increasingly good at training deep neural networks to learn a very accurate mapping from inputs to outputs, whether they are images, sentences, label predictions, etc. from large amounts of labeled data.
What our models still frightfully lack is the ability to generalize to conditions that are different from the ones encountered during training. When is this necessary? Every time you apply your model not to a carefully constructed dataset but to the real world.</description>
    </item>
    
    <item>
      <title>Deconstructing Deep Meta Learning</title>
      <link>https://obaydakov.github.io/post/2017/deconstructing-deep-meta-learning/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deconstructing-deep-meta-learning/</guid>
      <description>This article explores in more detail the idea of Meta Learning that was previously introduced in a post &amp;ldquo;The Meta Model and Meta Meta Model of Deep Learning&amp;rdquo;. In this post, I explore &amp;ldquo;Learning to Learn&amp;rdquo; as a Meta Learning approach. We have to be very careful to distinguish between Learning to Learn and Hyper Parameter Optimization (HPO). HPO and more generally searching for architectures differs from &amp;ldquo;learning to learn&amp;rdquo; in that that HPO explores the space of architectures while meta-learning explores the space of learning algorithms.</description>
    </item>
    
    <item>
      <title>Spark ML and Scala</title>
      <link>https://obaydakov.github.io/post/2017/spark-ml-and-scala/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/spark-ml-and-scala/</guid>
      <description>Usefull links:
Spark MLLib - Predict Store Sales with ML Pipelines
sparktutorials.net
KeystoneML
Streamline the Machine Learning Process Using Apache Spark ML Pipelines
Spam Detection with Sparkling Water and Spark Machine Learning Pipelines
Extend Spark ML for your own model/transformer types
Spam classification using Spark&amp;rsquo;s DataFrames, ML and Zeppelin (Part 1)
How-to: Predict Telco Churn with Apache Spark MLlib
Feature Extraction and Transformation - RDD-based API
Spark Machine Learning Pipeline by Example</description>
    </item>
    
    <item>
      <title>State of Hyperparameter Selection</title>
      <link>https://obaydakov.github.io/post/2017/state-of-hyperparameter-selection/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/state-of-hyperparameter-selection/</guid>
      <description>Historically hyperparameter determination has been a woefully forgotten aspect of machine learning. With the rise of neural nets - which require more hyperparameters, more precisely tuned than many other models - there has been a recent surge of interest in intelligent methods for selection; however, the average practitioner still seems to commonly use either default hyperparameters, grid search, random search, or (believe it or not) manual search.
For the readers who don&#39;t know, hyperparameter selection boils down to a conceptually simple problem: you have a set of variables (your hyperparameters) and an objective function (a measure of how good your model is).</description>
    </item>
    
    <item>
      <title>How to Use t-SNE Effectively</title>
      <link>https://obaydakov.github.io/post/2017/how-to-use-t-sne-effectively/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/how-to-use-t-sne-effectively/</guid>
      <description>A popular method for exploring high-dimensional data is something called t-SNE, introduced by van der Maaten and Hinton in 2008 [1]. The technique has become widespread in the field of machine learning, since it has an almost magical ability to create compelling two-dimensonal &amp;ldquo;maps&amp;rdquo; from data with hundreds or even thousands of dimensions.
Useful links:
 How to Use t-SNE Effectively
 Dissect t-SNE
 Dimensionality reduction: PCA, MDS, t-SNE</description>
    </item>
    
    <item>
      <title>Spark and XGBoost using Scala</title>
      <link>https://obaydakov.github.io/post/2017/spark-and-xgboost-using-scala/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/spark-and-xgboost-using-scala/</guid>
      <description>Recently XGBoost project released a package on github where it is included interface to scala, java and spark (more info at this link).
I would like to run xgboost on a big set of data.
In this post I just report the scala code lines which can be useful to run spark and xgboost.
In a further post I&amp;rsquo;m going to show the software setup and the integration of this project in Itellij IDEA community edition IDE.</description>
    </item>
    
    <item>
      <title>The 5 key challenges to building a successful data team</title>
      <link>https://obaydakov.github.io/post/2017/the-5-key-challenges-to-building-a-successful-data-team/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/the-5-key-challenges-to-building-a-successful-data-team/</guid>
      <description>The age of data is here. Sensors, cameras, security monitoring systems, software, hardware, the internet, and even humans themselves all have one thing in common: data. And as data becomes ubiquitous, in parallel, business interest in using and learning from that data is on the rise. Enter big data, a holistic term that aims to encapsulate the sheer massiveness of the amount of information available from the growing number of data sources.</description>
    </item>
    
    <item>
      <title>Finding &#34;Gems&#34; in Big Data</title>
      <link>https://obaydakov.github.io/post/2017/finding-gems-in-big-data/</link>
      <pubDate>Sun, 02 Apr 2017 18:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/finding-gems-in-big-data/</guid>
      <description>In addition to traditional statistical arrows like cluster analysis, principal components, and k-nearest neighbor, Indurkhya, in his course, also teaches non-standard methods.
One such method is based in information theory and relies on measures of data complexity. The idea is to isolate the records that add the most complexity to the data. An intuitive way to think about and measure this, without getting too deep into information theory, is to consider data compression algorithms.</description>
    </item>
    
    <item>
      <title>Deep Feature Synthesis: Towards Automating Data Science Endeavors</title>
      <link>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</guid>
      <description>Abstract-In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach.</description>
    </item>
    
    <item>
      <title>Deep Learning &amp; Parameter Tuning with MXnet, H2o Package in R</title>
      <link>https://obaydakov.github.io/post/2017/deep-learning-parameter-tuning-with-mxnet-h2o-package-in-r/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-learning-parameter-tuning-with-mxnet-h2o-package-in-r/</guid>
      <description>Introduction Deep Learning isn&amp;rsquo;t a recent discovery. The seeds were sown back in the 1950s when the first artificial neural network was created. Since then, progress has been rapid, with the structure of the neuron being &amp;ldquo;re-invented&amp;rdquo; artificially.
Computers and mobiles have now become powerful enough to identify objects from images.
Not just images, they can chat with you as well! Haven&amp;rsquo;t you tried Google&amp;rsquo;s Allo app ? That&amp;rsquo;s not all-they can drive, make supersonic calculations, and help businesses solve the most complicated problems (more users, revenue, etc).</description>
    </item>
    
    <item>
      <title>Feature Engineering &#43; H2o Gradient Boosting (GBM) in R Scores 0.936</title>
      <link>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</guid>
      <description>Load data and explore Data Pre-processing Dropped Features One Hot Encoding Feature Engineering Model Training  Link</description>
    </item>
    
    <item>
      <title>Spark ML</title>
      <link>https://obaydakov.github.io/post/2017/spark-ml/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/spark-ml/</guid>
      <description>Spark MLLib - Predict Store Sales with ML Pipelines
sparktutorials.net
KeystoneML
Streamline the Machine Learning Process Using Apache Spark ML Pipelines
Spam Detection with Sparkling Water and Spark Machine Learning Pipelines
Extend Spark ML for your own model/transformer types
Spam classification using Spark&amp;rsquo;s DataFrames, ML and Zeppelin (Part 1)
How-to: Predict Telco Churn with Apache Spark MLlib
Feature Extraction and Transformation - RDD-based API
Spark Machine Learning Pipeline by Example</description>
    </item>
    
    <item>
      <title>Tutorial on XGBoost and Parameter Tuning in R</title>
      <link>https://obaydakov.github.io/post/2017/tutorial-on-xgboost-and-parameter-tuning-in-r/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/tutorial-on-xgboost-and-parameter-tuning-in-r/</guid>
      <description>Table of Contents  What is XGBoost? Why is it so good? How does XGBoost work? Understanding XGBoost Tuning Parameters Practical - Tuning XGBoost using R  Link</description>
    </item>
    
    <item>
      <title>Using TensorFlow with R</title>
      <link>https://obaydakov.github.io/post/2017/using-tensorflow-with-r/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/using-tensorflow-with-r/</guid>
      <description>TensorFlowT is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google&amp;rsquo;s Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.</description>
    </item>
    
    <item>
      <title>An Efficient Approach for Assessing Hyperparameter Importance</title>
      <link>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</link>
      <pubDate>Sat, 01 Apr 2017 17:11:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</guid>
      <description>The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization.</description>
    </item>
    
    <item>
      <title>Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA</title>
      <link>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</guid>
      <description>WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKAs learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method.</description>
    </item>
    
    <item>
      <title>Efficient and Robust Automated Machine Learning</title>
      <link>https://obaydakov.github.io/post/2017/efficient-and-robust-automated-machine-learning/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/efficient-and-robust-automated-machine-learning/</guid>
      <description>The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters.
Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://obaydakov.github.io/post/2017/2017-04-02-finding-gems-in-big-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/2017-04-02-finding-gems-in-big-data/</guid>
      <description>Finding Gems in Big Datacode{white-space: pre;}pre:not([class]) {background-color: white;}if (window.hljs &amp;&amp; document.readyState &amp;&amp; document.readyState === &#34;complete&#34;) {window.setTimeout(function() {hljs.initHighlighting();}, 0);}h1 {font-size: 34px;}h1.title {font-size: 38px;}h2 {font-size: 30px;}h3 {font-size: 24px;}h4 {font-size: 18px;}h5 {font-size: 16px;}h6 {font-size: 12px;}.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://obaydakov.github.io/post/2017/2017-07-19-hortonworks-data-flow-3-streamig-applications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/2017-07-19-hortonworks-data-flow-3-streamig-applications/</guid>
      <description>&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD title: Hortonworks Data Flow 3 - streamig applications author: Oleg Baydakov date: &amp;lsquo;2017-07-19&amp;rsquo; slug: hortonworks-data-flow-3-streamig-applications categories: [] tags: [] description: &amp;ldquo;
thumbnail: &amp;ldquo; Hortonworks, Inc.  (NASDAQ: HDP), a leading innovator of open and connected data platforms, today announced the general availability of Hortonworks DataFlow (HDFT) 3.0, the next generation of its open source data-in-motion platform. HDF enables customers to collect, curate, analyze and act on all data in real-time, across the data center and cloud.</description>
    </item>
    
  </channel>
</rss>