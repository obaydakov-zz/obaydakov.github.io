<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Oleg Baydakov</title>
    <link>https://obaydakov.github.io/post/</link>
    <description>Recent content in Posts on Oleg Baydakov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Sep 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://obaydakov.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Golang development</title>
      <link>https://obaydakov.github.io/post/2017/golang-development/</link>
      <pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/golang-development/</guid>
      <description>Video</description>
    </item>
    
    <item>
      <title>An Efficient Approach for Assessing Hyperparameter Importance</title>
      <link>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</link>
      <pubDate>Sat, 01 Apr 2017 17:11:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</guid>
      <description>The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization.</description>
    </item>
    
  </channel>
</rss>