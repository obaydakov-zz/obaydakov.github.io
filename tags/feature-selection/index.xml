<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feature Selection on Oleg Baydakov</title>
    <link>https://obaydakov.github.io/tags/feature-selection/</link>
    <description>Recent content in Feature Selection on Oleg Baydakov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jan 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://obaydakov.github.io/tags/feature-selection/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Understanding Feature Engineering (Part 3) — Traditional Methods for Text Data</title>
      <link>https://obaydakov.github.io/post/2018/understanding-feature-engineering-part-3-traditional-methods-for-text-data/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/understanding-feature-engineering-part-3-traditional-methods-for-text-data/</guid>
      <description>Traditional strategies for taming unstructured, textual data</description>
    </item>
    
    <item>
      <title>What do you mean by 1D, 2D and 3D Convolutions in CNN?</title>
      <link>https://obaydakov.github.io/post/2018/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Feature Engineering</title>
      <link>https://obaydakov.github.io/post/2018/understanding-feature-engineering/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/understanding-feature-engineering/</guid>
      <description>Continuous Numeric Data
Categorical Data</description>
    </item>
    
    <item>
      <title>Ensemble Learning to Improve Machine Learning Results</title>
      <link>https://obaydakov.github.io/post/2017/ensemble-learning-to-improve-machine-learning-results/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/ensemble-learning-to-improve-machine-learning-results/</guid>
      <description>Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).
Ensemble methods can be divided into two groups:
sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost).
The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.</description>
    </item>
    
    <item>
      <title>Spark and XGBoost using Scala</title>
      <link>https://obaydakov.github.io/post/2017/spark-and-xgboost-using-scala/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/spark-and-xgboost-using-scala/</guid>
      <description>Recently XGBoost project released a package on github where it is included interface to scala, java and spark (more info at this link).
I would like to run xgboost on a big set of data.
In this post I just report the scala code lines which can be useful to run spark and xgboost.
In a further post I&amp;rsquo;m going to show the software setup and the integration of this project in Itellij IDEA community edition IDE.</description>
    </item>
    
    <item>
      <title>Deep Feature Synthesis: Towards Automating Data Science Endeavors</title>
      <link>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</guid>
      <description>Abstract-In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach.</description>
    </item>
    
    <item>
      <title>Feature Engineering &#43; H2o Gradient Boosting (GBM) in R Scores 0.936</title>
      <link>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</guid>
      <description>Load data and explore Data Pre-processing Dropped Features One Hot Encoding Feature Engineering Model Training  Link</description>
    </item>
    
    <item>
      <title>Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA</title>
      <link>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</guid>
      <description>WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA’s learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method.</description>
    </item>
    
  </channel>
</rss>