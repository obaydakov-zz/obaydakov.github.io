<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on Oleg Baydakov</title>
    <link>https://obaydakov.github.io/categories/algorithms/</link>
    <description>Recent content in Algorithms on Oleg Baydakov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Dec 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://obaydakov.github.io/categories/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Backpropagation Example</title>
      <link>https://obaydakov.github.io/post/2017/backpropagation-example/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/backpropagation-example/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ensemble learning - model stacking</title>
      <link>https://obaydakov.github.io/post/2017/ensemble-learning-model-stacking/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/ensemble-learning-model-stacking/</guid>
      <description>Introduction

Stacking (sometimes called &amp;ldquo;stacked generalization&amp;rdquo;) involves training a learning algorithm to combine the predictions of several other learning algorithms.
 First, all of the other algorithms are trained using the available data
 Then a combiner algorithm, the metalearner, is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary metalearning algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used for metalearning.</description>
    </item>
    
    <item>
      <title>Clustering Algorithms: On Learning, Validation, Performance</title>
      <link>https://obaydakov.github.io/post/clustering-algorithms-on-learning-validation-performance/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/clustering-algorithms-on-learning-validation-performance/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Credit Card Fraud Detection using Autoencoders in Keras</title>
      <link>https://obaydakov.github.io/post/credit-card-fraud-detection-using-autoencoders-in-keras/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/credit-card-fraud-detection-using-autoencoders-in-keras/</guid>
      <description>we will train an Autoencoder Neural Network (implemented in Keras) in unsupervised (or semi-supervised) fashion for Anomaly Detection in credit card transaction data. The trained model will be evaluated on pre-labeled and anonymized dataset.
Link
Github</description>
    </item>
    
    <item>
      <title>Microservice Architecture</title>
      <link>https://obaydakov.github.io/post/2017/microservice-architecture/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/microservice-architecture/</guid>
      <description>What are microservices?
Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of loosely coupled services, which implement business capabilities. The microservice architecture enables the continuous delivery/deployment of large, complex applications. It also enables an organization to evolve its technology stack.
Microservices are not a silver bullet
The microservice architecture is not a silver bullet. It has several drawbacks. Moreover, when using this architecture there are numerous issues that you must address.</description>
    </item>
    
    <item>
      <title>Fast Data with Apache Flink at ING lessons learned from designing and building a large streaming </title>
      <link>https://obaydakov.github.io/post/2017/fast-data-with-apache-flink-at-ing-lessons-learned-from-designing-and-building-a-large-streaming/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/fast-data-with-apache-flink-at-ing-lessons-learned-from-designing-and-building-a-large-streaming/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Designing a Microservices Architecture for Failure</title>
      <link>https://obaydakov.github.io/post/2017/designing-a-microservices-architecture-for-failure/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/designing-a-microservices-architecture-for-failure/</guid>
      <description>A Microservices architecture makes it possible to isolate failures through well-defined service boundaries. But like in every distributed system, there is a higher chance for network, hardware or application level issues. As a consequence of service dependencies, any component can be temporarily unavailable for their consumers. To minimize the impact of partial outages we need to build fault tolerant services that can gracefully respond to certain types of outages.
Link</description>
    </item>
    
    <item>
      <title>Explore Predictive Maintenance with flexdashboard</title>
      <link>https://obaydakov.github.io/post/2017/explore-predictive-maintenance-with-flexdashboard/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/explore-predictive-maintenance-with-flexdashboard/</guid>
      <description>Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below).
A common use case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime.</description>
    </item>
    
    <item>
      <title>Ensemble Learning to Improve Machine Learning Results</title>
      <link>https://obaydakov.github.io/post/2017/ensemble-learning-to-improve-machine-learning-results/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/ensemble-learning-to-improve-machine-learning-results/</guid>
      <description>Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).
Ensemble methods can be divided into two groups:
sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost).
The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.</description>
    </item>
    
    <item>
      <title>Visualizing Concurrency in Go Posted on Jan 24, 2016</title>
      <link>https://obaydakov.github.io/post/2017/visualizing-concurrency-in-go-posted-on-jan-24-2016/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/visualizing-concurrency-in-go-posted-on-jan-24-2016/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>Go Concurrency Patterns: Pipelines and cancellation</title>
      <link>https://obaydakov.github.io/post/2017/go-concurrency-patterns-pipelines-and-cancellation/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/go-concurrency-patterns-pipelines-and-cancellation/</guid>
      <description>Introduction
Go&amp;rsquo;s concurrency primitives make it easy to construct streaming data pipelines that make efficient use of I/O and multiple CPUs. This article presents examples of such pipelines, highlights subtleties that arise when operations fail, and introduces techniques for dealing with failures cleanly.
What is a pipeline?
There&amp;rsquo;s no formal definition of a pipeline in Go; it&amp;rsquo;s just one of many kinds of concurrent programs. Informally, a pipeline is a series of stages connected by channels, where each stage is a group of goroutines running the same function.</description>
    </item>
    
    <item>
      <title>Non-standard clusterization - Growing Neural Gas</title>
      <link>https://obaydakov.github.io/post/non-standard-clusterization-growing-neural-gas/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/non-standard-clusterization-growing-neural-gas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Learning Cheatsheets</title>
      <link>https://obaydakov.github.io/post/2017/machine-learning-cheatsheets/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/machine-learning-cheatsheets/</guid>
      <description>Link</description>
    </item>
    
    <item>
      <title>An overview of gradient descent optimization algorithms</title>
      <link>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</link>
      <pubDate>Fri, 21 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</guid>
      <description>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne&amp;rsquo;s, caffe&amp;rsquo;s, and keras&amp;rsquo; documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.
Link</description>
    </item>
    
    <item>
      <title>Credit Risk Prediction</title>
      <link>https://obaydakov.github.io/post/2017/credit-risk-prediction/</link>
      <pubDate>Sun, 16 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/credit-risk-prediction/</guid>
      <description>Credit Risk Scoring is a classic yet still increasingly important operation in banking as banks continue to be increasingly risk careful when lending for mortgages or commercial purposes, in an industry known for fierce competition and the global financial crisis. With an accurate credit risk scoring model a bank is able to predict the likelihood of default on a transaction. This will in turn help evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt, as well as determine who qualifies for a loan, at what interest rate, and what credit limits, and even determine which customers are likely to bring in the most revenue through a variety of products.</description>
    </item>
    
    <item>
      <title>The Algorithms Behind Probabilistic Programming</title>
      <link>https://obaydakov.github.io/post/2017/the-algorithms-behind-probabilistic-programming/</link>
      <pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/the-algorithms-behind-probabilistic-programming/</guid>
      <description>Bayesian Inference
Probabilistic programming enables us to construct and fit probabilistic models in code. At its essence, Bayesian inference is a principled way to draw conclusions from incomplete or imperfect data, by interpreting data in light of prior knowledge of probabilities. As pretty much all real-world data is incomplete or imperfect in some way, it&amp;rsquo;s an important (and old!) idea.
Bayesian inference might be the way to go if you:</description>
    </item>
    
    <item>
      <title>Algorithms and Data Structures</title>
      <link>https://obaydakov.github.io/post/2017/algorithms-and-data-structures/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/algorithms-and-data-structures/</guid>
      <description>Algorithms and Data Structures
This is the collection of algorithms, data structures and Interview Questions with solutions. This repository contains my solutions for common algorithmic problems and implementation of Data Structures in Java. I&amp;rsquo;ve created this repository to learn about algorithms. I am adding solutions continuously. Link</description>
    </item>
    
    <item>
      <title>Kalman filter</title>
      <link>https://obaydakov.github.io/post/2017/kalman-filter/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/kalman-filter/</guid>
      <description>This article provides a simple and intuitive derivation of the Kalman filter, with the aim of teaching this useful tool to students from disciplines that do not require a strong mathematical background. The most complicated level of mathematics required to understand this derivation is the ability to multiply two Gaussian functions together and reduce the result to a compact form. Link</description>
    </item>
    
    <item>
      <title>Feature Engineering &#43; H2o Gradient Boosting (GBM) in R Scores 0.936</title>
      <link>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/feature-engineering-h2o-gradient-boosting-gbm-in-r-scores-0-936/</guid>
      <description>Load data and explore Data Pre-processing Dropped Features One Hot Encoding Feature Engineering Model Training  Link</description>
    </item>
    
    <item>
      <title>Tutorial on XGBoost and Parameter Tuning in R</title>
      <link>https://obaydakov.github.io/post/2017/tutorial-on-xgboost-and-parameter-tuning-in-r/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/tutorial-on-xgboost-and-parameter-tuning-in-r/</guid>
      <description>Table of Contents  What is XGBoost? Why is it so good? How does XGBoost work? Understanding XGBoost Tuning Parameters Practical - Tuning XGBoost using R  Link</description>
    </item>
    
  </channel>
</rss>