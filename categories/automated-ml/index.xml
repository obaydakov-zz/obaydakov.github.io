<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Automated Ml on Oleg Baydakov</title>
    <link>https://obaydakov.github.io/categories/automated-ml/</link>
    <description>Recent content in Automated Ml on Oleg Baydakov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Jan 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://obaydakov.github.io/categories/automated-ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python Data Science Handbook</title>
      <link>https://obaydakov.github.io/post/2018/python-data-science-handbook/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/python-data-science-handbook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Building a FAQ Chatbot in Python – The Future of Information Searching</title>
      <link>https://obaydakov.github.io/post/2018/building-a-faq-chatbot-in-python-the-future-of-information-searching/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/building-a-faq-chatbot-in-python-the-future-of-information-searching/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introducing Kubeflow - A Composable, Portable, Scalable ML Stack Built for Kubernetes</title>
      <link>https://obaydakov.github.io/post/2018/introducing-kubeflow-a-composable-portable-scalable-ml-stack-built-for-kubernetes/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/introducing-kubeflow-a-composable-portable-scalable-ml-stack-built-for-kubernetes/</guid>
      <description>Kubeflow: новый проект для работы с машинным обучением в Kubernetes
Kubernetes and ML</description>
    </item>
    
    <item>
      <title>Paddle Paddle</title>
      <link>https://obaydakov.github.io/post/2018/paddle-paddle/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/paddle-paddle/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Google Colaboratory and Azure notebook</title>
      <link>https://obaydakov.github.io/post/2018/google-colaboratory-and-azure-notebook/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/google-colaboratory-and-azure-notebook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Create a scalable REST API with Falcon and RHSCL</title>
      <link>https://obaydakov.github.io/post/2018/create-a-scalable-rest-api-with-falcon-and-rhscl/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/create-a-scalable-rest-api-with-falcon-and-rhscl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hyperdash Machine Learning Monitoring.  Worry-free tracking for your training.</title>
      <link>https://obaydakov.github.io/post/2018/hyperdash-machine-learning-monitoring-worry-free-tracking-for-your-training/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/hyperdash-machine-learning-monitoring-worry-free-tracking-for-your-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A gallery of interesting Jupyter Notebooks</title>
      <link>https://obaydakov.github.io/post/2018/a-gallery-of-interesting-jupyter-notebooks/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2018/a-gallery-of-interesting-jupyter-notebooks/</guid>
      <description>This page is a curated collection of Jupyter/IPython notebooks that are notable. Feel free to add new content here, but please try to only include links to notebooks that include interesting visual or technical content; this should not simply be a dump of a Google search on every ipynb file out there.
Important contribution instructions: If you add new content, please ensure that for any notebook you link to, the link is to the rendered version using nbviewer, rather than the raw file.</description>
    </item>
    
    <item>
      <title>Tutorial to deploy Machine Learning models in Production as APIs (using Flask)</title>
      <link>https://obaydakov.github.io/post/2017/tutorial-to-deploy-machine-learning-models-in-production-as-apis-using-flask/</link>
      <pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/tutorial-to-deploy-machine-learning-models-in-production-as-apis-using-flask/</guid>
      <description></description>
    </item>
    
    <item>
      <title>docker-stacks</title>
      <link>https://obaydakov.github.io/post/2017/docker-stacks/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/docker-stacks/</guid>
      <description>Opinionated stacks of ready-to-run Jupyter applications in Docker.
Link</description>
    </item>
    
    <item>
      <title>MICE: Multivariate Imputation by Chained Equations</title>
      <link>https://obaydakov.github.io/post/2017/mice-multivariate-imputation-by-chained-equations/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/mice-multivariate-imputation-by-chained-equations/</guid>
      <description>Capabilities of mice package
The mice package contains functions to
Inspect the missing data pattern Impute the missing data mm times, resulting in mm completed data sets Diagnose the quality of the imputed values Analyze each completed data set Pool the results of the repeated analyses Store and export the imputed data in various formats Generate simulated incomplete data Incorporate custom imputation methods Choose which cells to impute
Link
Handling Missing Data in R with mice</description>
    </item>
    
    <item>
      <title>EVERYTHING ARTIFICIAL INTELLIGENCE</title>
      <link>https://obaydakov.github.io/post/2017/everything-artificial-intelligence/</link>
      <pubDate>Wed, 22 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/everything-artificial-intelligence/</guid>
      <description>Link
H2O Auto ML
h2oai/h2o-tutorials
Ensembles: Stacking, Super Learner</description>
    </item>
    
    <item>
      <title>Build Realtime APIs</title>
      <link>https://obaydakov.github.io/post/2017/build-realtime-apis/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/build-realtime-apis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Go Data Science Tooling, Packages, Libraries, etc.</title>
      <link>https://obaydakov.github.io/post/2017/go-data-science-tooling-packages-libraries-etc/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/go-data-science-tooling-packages-libraries-etc/</guid>
      <description>This is a curated list of well-maintained and developing tools, packages, libraries, etc. related to doing data science with Go.
Link</description>
    </item>
    
    <item>
      <title>Machine Learning Explorations</title>
      <link>https://obaydakov.github.io/post/2017/machine-learning-explorations/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/machine-learning-explorations/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GETTING STARTED WITH Microservices</title>
      <link>https://obaydakov.github.io/post/2017/getting-started-with-microservices/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/getting-started-with-microservices/</guid>
      <description>The term “microservices” describes a new software development style that has grown from recent trends in software development/management practices—practices meant to increase the speed and efficiency of developing and managing software solutions at scale.
Agile methods, DevOps culture, cloud, Linux containers, and the widespread adoption (both culturally and technically) of CI/CD methods across the industry are making it possible to build truly modular largescale service-optimized systems for both internal and commercial use.</description>
    </item>
    
    <item>
      <title>Automated Machine Learning: Deploying AutoML to the Cloud</title>
      <link>https://obaydakov.github.io/post/2017/automated-machine-learning-deploying-automl-to-the-cloud/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/automated-machine-learning-deploying-automl-to-the-cloud/</guid>
      <description>[]https://content.pivotal.io/blog/automated-machine-learning-deploying-automl-to-the-cloud)
What exactly is AutoML? AutoML is a broad term and technically could encompass the entire data science cycle from data exploration to model building. However, I have found it most commonly refers to automating feature preprocessing and selection, model algorithm selection, and hyperparameter tuning–steps located at the end of the data science process.
 AutoML offers tangible benefits for model selection and optimization.  2.It&amp;rsquo;s very easy to get started.</description>
    </item>
    
    <item>
      <title>Machine Learning Application Skeleton</title>
      <link>https://obaydakov.github.io/post/2017/machine-learning-application-skeleton/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/machine-learning-application-skeleton/</guid>
      <description>In this blog post I provide an overview of a Python skeleton application I made. This skeleton can help you bridge the gap between your model and a machine learning application.
For example, you can use your existing Flask application, import it in run_app.py as app, and this will add the production ready features of Gunicorn.
Link
Code</description>
    </item>
    
    <item>
      <title>Real time analytics: Divolte &#43; Kafka &#43; Druid &#43; Superset</title>
      <link>https://obaydakov.github.io/post/2017/real-time-analytics-divolte-kafka-druid-superset/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/real-time-analytics-divolte-kafka-druid-superset/</guid>
      <description>In today&amp;rsquo;s world you want to learn from your customers as fast as possible. This blog gives an introduction to setting up streaming analytics using open source technologies. We&amp;rsquo;ll use Divolte, Kafka, Superset and Druid to set up a system that lets you instantaneously get a deeper understanding of your customers.
Having your analytics in a streaming fashion enable you to continuously analyze your customer&amp;rsquo;s behaviour and immediately take action on it.</description>
    </item>
    
    <item>
      <title>Real-Time Fraud Detection with Microservices</title>
      <link>https://obaydakov.github.io/post/2017/real-time-fraud-detection-with-microservices/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/real-time-fraud-detection-with-microservices/</guid>
      <description>

Requirements
 CloudFoundry (tested with pcfdev v0.21)
 Apache Geode (tested with v1.0.0-incubating.M2) - to be replaced by GemFire 9.0 when that is released
 PostgreSQL (tested with 9.5.3) - to be replaced by Greenplum server once GemFire 9.0 is released
 Spring Cloud Dataflow (tested with SCDF v1.1.0.M2, CloudFoundry Deployer v1.1.0.M1)
 Spark-redis (tested with v0.1.1) - installed in local maven repository
  </description>
    </item>
    
    <item>
      <title>An overview of gradient descent optimization algorithms</title>
      <link>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-overview-of-gradient-descent-optimization-algorithms/</guid>
      <description>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne&amp;rsquo;s, caffe&amp;rsquo;s, and keras&amp;rsquo; documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.
Link</description>
    </item>
    
    <item>
      <title>Model evaluation, model selection, and algorithm selection in machine learning  - Cross-validation and hyperparameter tuning </title>
      <link>https://obaydakov.github.io/post/2017/model-evaluation-model-selection-and-algorithm-selection-in-machine-learning-cross-validation-and-hyperparameter-tuning/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/model-evaluation-model-selection-and-algorithm-selection-in-machine-learning-cross-validation-and-hyperparameter-tuning/</guid>
      <description>Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset.</description>
    </item>
    
    <item>
      <title>Productionizing Apache SparkT MLlib Models for Real-time Prediction Serving</title>
      <link>https://obaydakov.github.io/post/2017/c-users-oleg-baydakov-documents-myblog-new-myblog-hugo/</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/c-users-oleg-baydakov-documents-myblog-new-myblog-hugo/</guid>
      <description>Data science and machine learning tools traditionally focus on training models. When companies begin to employ machine learning in actual production workflows, they encounter new sources of friction such as sharing models across teams, deploying identical models on different systems, and maintaining featurization logic. In this webinar, we discuss how Databricks provides a smooth path for productionizing Apache Spark MLlib models and featurization pipelines.
Databricks Model Scoring provides a simple API for exporting MLlib models and pipelines.</description>
    </item>
    
    <item>
      <title>Python framewrok - QML</title>
      <link>https://obaydakov.github.io/post/2017/python-framewrok-qml/</link>
      <pubDate>Wed, 19 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/python-framewrok-qml/</guid>
      <description>Description
Code</description>
    </item>
    
    <item>
      <title>Transfer Learning - Machine Learning&#39;s Next Frontier</title>
      <link>https://obaydakov.github.io/post/2017/transfer-learning-machine-learning-s-next-frontier/</link>
      <pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/transfer-learning-machine-learning-s-next-frontier/</guid>
      <description>Table of contents:
What is Transfer Learning? Why Transfer Learning Now? A Definition of Transfer Learning Transfer Learning Scenarios Applications of Transfer Learning Learning from simulations Adapting to new domains Transferring knowledge across languages Transfer Learning Methods Using pre-trained CNN features Learning domain-invariant representations Making representations more similar Confusing domains Related Research Areas Semi-supervised learning Using available data more effectively Improving models&amp;rsquo; ability to generalize Making models more robust Multi-task learning Continuous learning Zero-shot learning Conclusion</description>
    </item>
    
    <item>
      <title>Deconstructing Deep Meta Learning</title>
      <link>https://obaydakov.github.io/post/2017/deconstructing-deep-meta-learning/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deconstructing-deep-meta-learning/</guid>
      <description>This article explores in more detail the idea of Meta Learning that was previously introduced in a post &amp;ldquo;The Meta Model and Meta Meta Model of Deep Learning&amp;rdquo;. In this post, I explore &amp;ldquo;Learning to Learn&amp;rdquo; as a Meta Learning approach. We have to be very careful to distinguish between Learning to Learn and Hyper Parameter Optimization (HPO). HPO and more generally searching for architectures differs from &amp;ldquo;learning to learn&amp;rdquo; in that that HPO explores the space of architectures while meta-learning explores the space of learning algorithms.</description>
    </item>
    
    <item>
      <title>State of Hyperparameter Selection</title>
      <link>https://obaydakov.github.io/post/2017/state-of-hyperparameter-selection/</link>
      <pubDate>Wed, 05 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/state-of-hyperparameter-selection/</guid>
      <description>Historically hyperparameter determination has been a woefully forgotten aspect of machine learning. With the rise of neural nets - which require more hyperparameters, more precisely tuned than many other models - there has been a recent surge of interest in intelligent methods for selection; however, the average practitioner still seems to commonly use either default hyperparameters, grid search, random search, or (believe it or not) manual search.
For the readers who don&#39;t know, hyperparameter selection boils down to a conceptually simple problem: you have a set of variables (your hyperparameters) and an objective function (a measure of how good your model is).</description>
    </item>
    
    <item>
      <title>Deep Feature Synthesis: Towards Automating Data Science Endeavors</title>
      <link>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/deep-feature-synthesis-towards-automating-data-science-endeavors/</guid>
      <description>Abstract-In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach.</description>
    </item>
    
    <item>
      <title>An Efficient Approach for Assessing Hyperparameter Importance</title>
      <link>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</link>
      <pubDate>Sat, 01 Apr 2017 17:11:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/an-efficient-approach-for-assessing-hyperparameter-importance/</guid>
      <description>The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization.</description>
    </item>
    
    <item>
      <title>Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA</title>
      <link>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/auto-weka-2-0-automatic-model-selection-and-hyperparameter-optimization-in-weka/</guid>
      <description>WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA’s learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method.</description>
    </item>
    
    <item>
      <title>Efficient and Robust Automated Machine Learning</title>
      <link>https://obaydakov.github.io/post/2017/efficient-and-robust-automated-machine-learning/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://obaydakov.github.io/post/2017/efficient-and-robust-automated-machine-learning/</guid>
      <description>The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters.
Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods.</description>
    </item>
    
  </channel>
</rss>